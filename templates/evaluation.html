{% extends "base.html" %}

{% block title %}Evaluation - LLM Development Platform{% endblock %}

{% block content %}
<div class="row mb-4">
    <div class="col-md-8">
        <h1 class="display-5 fw-bold">
            <i data-feather="bar-chart" class="me-3"></i>
            Evaluation
        </h1>
        <p class="lead text-muted">Assess model performance and quality</p>
    </div>
    <div class="col-md-4 text-end">
        <button type="button" class="btn btn-primary" data-bs-toggle="modal" data-bs-target="#evalModal">
            <i data-feather="play" class="me-2"></i>
            Run Evaluation
        </button>
    </div>
</div>

<!-- Evaluation Results -->
{% if evaluations %}
<div class="row mb-4">
    <div class="col-12">
        <div class="card">
            <div class="card-header">
                <h5 class="mb-0">Recent Evaluations</h5>
            </div>
            <div class="card-body">
                <div class="table-responsive">
                    <table class="table table-hover">
                        <thead>
                            <tr>
                                <th>Evaluation</th>
                                <th>Model</th>
                                <th>Perplexity</th>
                                <th>BLEU Score</th>
                                <th>ROUGE Score</th>
                                <th>Diversity</th>
                                <th>Avg Length</th>
                                <th>Date</th>
                            </tr>
                        </thead>
                        <tbody>
                            {% for eval in evaluations %}
                                <tr>
                                    <td><strong>{{ eval.eval_name }}</strong></td>
                                    <td>{{ eval.model.name }}</td>
                                    <td>
                                        <span class="badge bg-{% if eval.perplexity < 20 %}success{% elif eval.perplexity < 35 %}warning{% else %}danger{% endif %}">
                                            {{ "%.2f"|format(eval.perplexity) if eval.perplexity }}
                                        </span>
                                    </td>
                                    <td>
                                        <span class="badge bg-{% if eval.bleu_score > 0.6 %}success{% elif eval.bleu_score > 0.3 %}warning{% else %}danger{% endif %}">
                                            {{ "%.3f"|format(eval.bleu_score) if eval.bleu_score }}
                                        </span>
                                    </td>
                                    <td>
                                        <span class="badge bg-{% if eval.rouge_score > 0.6 %}success{% elif eval.rouge_score > 0.3 %}warning{% else %}danger{% endif %}">
                                            {{ "%.3f"|format(eval.rouge_score) if eval.rouge_score }}
                                        </span>
                                    </td>
                                    <td>{{ "%.3f"|format(eval.response_diversity) if eval.response_diversity }}</td>
                                    <td>{{ "%.1f"|format(eval.avg_response_length) if eval.avg_response_length }}</td>
                                    <td>{{ eval.created_at.strftime('%m/%d %H:%M') }}</td>
                                </tr>
                            {% endfor %}
                        </tbody>
                    </table>
                </div>
            </div>
        </div>
    </div>
</div>

<!-- Metrics Dashboard -->
<div class="row mb-4">
    <div class="col-lg-6">
        <div class="card">
            <div class="card-header">
                <h6 class="mb-0">Performance Metrics</h6>
            </div>
            <div class="card-body">
                <canvas id="metricsChart" width="400" height="200"></canvas>
            </div>
        </div>
    </div>
    <div class="col-lg-6">
        <div class="card">
            <div class="card-header">
                <h6 class="mb-0">Model Comparison</h6>
            </div>
            <div class="card-body">
                <canvas id="comparisonChart" width="400" height="200"></canvas>
            </div>
        </div>
    </div>
</div>
{% endif %}

<!-- Metrics Information -->
<div class="row">
    <div class="col-lg-8">
        <div class="card">
            <div class="card-header">
                <h6 class="mb-0">
                    <i data-feather="info" class="me-2"></i>
                    Evaluation Metrics Explained
                </h6>
            </div>
            <div class="card-body">
                <div class="row">
                    <div class="col-md-6 mb-3">
                        <h6>Perplexity</h6>
                        <p class="small text-muted">Measures how well the model predicts text. Lower values indicate better performance. Good models typically have perplexity between 10-30.</p>
                    </div>
                    <div class="col-md-6 mb-3">
                        <h6>BLEU Score</h6>
                        <p class="small text-muted">Evaluates text quality by comparing generated text to reference text. Ranges from 0-1, with higher values indicating better quality.</p>
                    </div>
                    <div class="col-md-6 mb-3">
                        <h6>ROUGE Score</h6>
                        <p class="small text-muted">Measures overlap between generated and reference text. Particularly useful for summarization tasks. Higher values are better.</p>
                    </div>
                    <div class="col-md-6 mb-3">
                        <h6>Response Diversity</h6>
                        <p class="small text-muted">Measures how varied the model's outputs are. Higher diversity indicates the model generates more creative and varied responses.</p>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <div class="col-lg-4">
        <div class="card">
            <div class="card-header">
                <h6 class="mb-0">Available Models</h6>
            </div>
            <div class="card-body">
                {% if models %}
                    <div class="list-group list-group-flush">
                        {% for model in models %}
                            <div class="list-group-item d-flex justify-content-between align-items-center px-0">
                                <div>
                                    <h6 class="mb-1">{{ model.name }}</h6>
                                    <small class="text-muted">{{ model.base_model }}</small>
                                </div>
                                <span class="badge bg-{% if model.status.value == 'available' %}success{% elif model.status.value == 'training' %}warning{% else %}secondary{% endif %}">
                                    {{ model.status.value }}
                                </span>
                            </div>
                        {% endfor %}
                    </div>
                {% else %}
                    <div class="text-center py-4">
                        <i data-feather="layers" class="text-muted mb-2" style="width: 48px; height: 48px;"></i>
                        <p class="text-muted">No models available for evaluation.</p>
                        <a href="{{ url_for('models') }}" class="btn btn-sm btn-outline-primary">Create Model</a>
                    </div>
                {% endif %}
            </div>
        </div>
    </div>
</div>

<!-- Evaluation Modal -->
<div class="modal fade" id="evalModal" tabindex="-1" aria-labelledby="evalModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <form method="POST" action="{{ url_for('run_evaluation') }}">
                <div class="modal-header">
                    <h5 class="modal-title" id="evalModalLabel">Run Model Evaluation</h5>
                    <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
                </div>
                <div class="modal-body">
                    <div class="mb-3">
                        <label for="model_id" class="form-label">Select Model</label>
                        <select class="form-select" id="model_id" name="model_id" required>
                            <option value="">Choose a model to evaluate...</option>
                            {% for model in models %}
                                <option value="{{ model.id }}">{{ model.name }} ({{ model.base_model }})</option>
                            {% endfor %}
                        </select>
                    </div>
                    <div class="mb-3">
                        <label for="eval_name" class="form-label">Evaluation Name</label>
                        <input type="text" class="form-control" id="eval_name" name="eval_name" required 
                               placeholder="e.g., Quality Assessment v1.0">
                    </div>
                    <div class="alert alert-info">
                        <i data-feather="info" class="me-2"></i>
                        <strong>Note:</strong> This evaluation will run standard benchmarks including perplexity, BLEU, ROUGE, and diversity metrics. The process typically takes 2-3 minutes.
                    </div>
                </div>
                <div class="modal-footer">
                    <button type="button" class="btn btn-secondary" data-bs-dismiss="modal">Cancel</button>
                    <button type="submit" class="btn btn-primary">
                        <i data-feather="play" class="me-2"></i>
                        Start Evaluation
                    </button>
                </div>
            </form>
        </div>
    </div>
</div>
{% endblock %}

{% block scripts %}
{% if evaluations %}
<script>
// Metrics Chart
const metricsCtx = document.getElementById('metricsChart').getContext('2d');
const metricsChart = new Chart(metricsCtx, {
    type: 'radar',
    data: {
        labels: ['Perplexity (inv)', 'BLEU Score', 'ROUGE Score', 'Diversity', 'Consistency'],
        datasets: [{
            label: 'Latest Evaluation',
            data: [
                {{ (100 - evaluations[0].perplexity) / 100 if evaluations[0].perplexity else 0.5 }},
                {{ evaluations[0].bleu_score if evaluations[0].bleu_score else 0.5 }},
                {{ evaluations[0].rouge_score if evaluations[0].rouge_score else 0.5 }},
                {{ evaluations[0].response_diversity if evaluations[0].response_diversity else 0.5 }},
                0.7
            ],
            backgroundColor: 'rgba(13, 202, 240, 0.2)',
            borderColor: 'rgb(13, 202, 240)',
            pointBackgroundColor: 'rgb(13, 202, 240)',
            pointBorderColor: '#fff',
            pointHoverBackgroundColor: '#fff',
            pointHoverBorderColor: 'rgb(13, 202, 240)'
        }]
    },
    options: {
        responsive: true,
        scales: {
            r: {
                beginAtZero: true,
                max: 1
            }
        },
        plugins: {
            legend: {
                display: false
            }
        }
    }
});

// Comparison Chart
const comparisonCtx = document.getElementById('comparisonChart').getContext('2d');
const comparisonChart = new Chart(comparisonCtx, {
    type: 'bar',
    data: {
        labels: [{% for eval in evaluations[:5] %}'{{ eval.model.name[:10] }}'{% if not loop.last %},{% endif %}{% endfor %}],
        datasets: [
            {
                label: 'BLEU Score',
                data: [{% for eval in evaluations[:5] %}{{ eval.bleu_score if eval.bleu_score else 0 }}{% if not loop.last %},{% endif %}{% endfor %}],
                backgroundColor: 'rgba(255, 193, 7, 0.6)',
                borderColor: 'rgb(255, 193, 7)',
                borderWidth: 1
            },
            {
                label: 'ROUGE Score',
                data: [{% for eval in evaluations[:5] %}{{ eval.rouge_score if eval.rouge_score else 0 }}{% if not loop.last %},{% endif %}{% endfor %}],
                backgroundColor: 'rgba(25, 135, 84, 0.6)',
                borderColor: 'rgb(25, 135, 84)',
                borderWidth: 1
            }
        ]
    },
    options: {
        responsive: true,
        scales: {
            y: {
                beginAtZero: true,
                max: 1
            }
        }
    }
});
</script>
{% endif %}
{% endblock %}
