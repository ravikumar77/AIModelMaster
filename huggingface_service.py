"""
Hugging Face Integration Service
Handles model uploads, authentication, and repository management
"""
import os
import json
import logging
import tempfile
import shutil
from pathlib import Path
from typing import Dict, Optional, Any, List
from datetime import datetime

logger = logging.getLogger(__name__)

class HuggingFaceService:
    def __init__(self):
        self.hf_token = os.environ.get("HUGGINGFACE_TOKEN")
        self.username = os.environ.get("HUGGINGFACE_USERNAME", "your-username")
        
    def is_authenticated(self) -> bool:
        """Check if HuggingFace token is available"""
        return self.hf_token is not None
    
    def upload_model(self, model_id: int, model_name: str, model_config: Dict) -> Dict[str, Any]:
        """Upload a model to HuggingFace Hub"""
        if not self.is_authenticated():
            return {
                "success": False,
                "error": "HuggingFace token not configured. Please set HUGGINGFACE_TOKEN environment variable.",
                "instructions": "Get your token from https://huggingface.co/settings/tokens"
            }
        
        try:
            # For now, return instructions since actual transformers library isn't available
            repo_name = f"{self.username}/{model_name.lower().replace(' ', '-')}"
            
            upload_script = self._generate_upload_script(model_name, model_config, repo_name)
            readme = self._generate_model_card(model_name, model_config)
            
            return {
                "success": True,
                "repo_name": repo_name,
                "repo_url": f"https://huggingface.co/{repo_name}",
                "upload_script": upload_script,
                "model_card": readme,
                "instructions": "Execute the upload script to push your model to HuggingFace Hub"
            }
            
        except Exception as e:
            logger.error(f"Error uploading model: {e}")
            return {
                "success": False,
                "error": f"Upload failed: {str(e)}"
            }
    
    def _generate_upload_script(self, model_name: str, config: Dict, repo_name: str) -> str:
        """Generate Python script for uploading model"""
        temperature = config.get('temperature', 0.7)
        max_length = config.get('max_length', 100)
        
        script = f"""#!/usr/bin/env python3
# HuggingFace Hub Upload Script for {model_name}
# Generated by LLM Development Platform

import os
import json
import shutil
from pathlib import Path

# Check if required packages are installed
try:
    from huggingface_hub import HfApi, login
    print("All required packages are available")
except ImportError as e:
    print(f"Missing required packages: {e}")
    print("Install with: pip install huggingface_hub")
    exit(1)

# Configuration
MODEL_NAME = "{model_name}"
REPO_NAME = "{repo_name}"
HF_TOKEN = os.environ.get("HUGGINGFACE_TOKEN")

if not HF_TOKEN:
    print("HUGGINGFACE_TOKEN environment variable not set")
    print("Get your token from: https://huggingface.co/settings/tokens")
    exit(1)

def main():
    print(f"Uploading {MODEL_NAME} to {REPO_NAME}")
    
    # Login to HuggingFace
    login(token=HF_TOKEN)
    print("Logged in to HuggingFace")
    
    # Create API instance
    api = HfApi()
    
    # Create repository if it doesn't exist
    try:
        api.create_repo(repo_id=REPO_NAME, private=False, exist_ok=True)
        print(f"Repository {REPO_NAME} ready")
    except Exception as e:
        print(f"Failed to create repository: {e}")
        return
    
    # Create temporary directory for model files
    temp_dir = Path("./temp_model_upload")
    temp_dir.mkdir(exist_ok=True)
    
    try:
        # Generate model configuration
        model_config = {{
            "model_type": "gpt2",
            "vocab_size": 50257,
            "architectures": ["GPT2LMHeadModel"],
            "task_specific_params": {{
                "text-generation": {{
                    "temperature": {temperature},
                    "max_length": {max_length},
                    "do_sample": True
                }}
            }}
        }}
        
        # Save config.json
        with open(temp_dir / "config.json", "w") as f:
            json.dump(model_config, f, indent=2)
        print("Generated config.json")
        
        # Create README.md
        readme_content = f'''---
license: mit
language: en
tags:
- text-generation
- fine-tuned
- llm-platform
---

# {model_name}

This model was fine-tuned using the LLM Development Platform.

## Model Details
- Base Model: GPT-2
- Fine-tuning Method: LoRA (Low-Rank Adaptation)
- Training Platform: LLM Development Platform

## Training Details
- Temperature: {temperature}
- Max Length: {max_length}

---
*Generated by LLM Development Platform*
'''
        
        with open(temp_dir / "README.md", "w") as f:
            f.write(readme_content)
        print("Generated README.md")
        
        # Upload files to repository
        api.upload_folder(
            folder_path=str(temp_dir),
            repo_id=REPO_NAME,
            repo_type="model",
            commit_message="Upload fine-tuned model from LLM Platform"
        )
        
        print(f"Successfully uploaded to: https://huggingface.co/{REPO_NAME}")
        
    except Exception as e:
        print(f"Upload failed: {e}")
    finally:
        # Cleanup
        if temp_dir.exists():
            shutil.rmtree(temp_dir)
        print("Cleanup completed")

if __name__ == "__main__":
    main()
"""
        return script
    
    def _generate_model_card(self, model_name: str, config: Dict) -> str:
        """Generate model card for HuggingFace"""
        return f"""---
license: mit
language: en
tags:
- text-generation
- fine-tuned
- llm-platform
pipeline_tag: text-generation
---

# {model_name}

This model was fine-tuned using the LLM Development Platform.

## Model Details
- **Model Type**: Fine-tuned Language Model
- **Base Model**: GPT-2
- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)
- **Training Platform**: LLM Development Platform
- **Created**: {datetime.now().strftime('%Y-%m-%d')}

## Intended Use
This model is designed for text generation tasks and has been fine-tuned for improved performance on specific domains.

## Usage

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("your-username/{model_name.lower().replace(' ', '-')}")
model = AutoModelForCausalLM.from_pretrained("your-username/{model_name.lower().replace(' ', '-')}")

# Generate text
input_text = "Your prompt here"
inputs = tokenizer.encode(input_text, return_tensors="pt")

# Generate with custom parameters
outputs = model.generate(
    inputs,
    max_length={config.get('max_length', 100)},
    temperature={config.get('temperature', 0.7)},
    do_sample=True,
    pad_token_id=tokenizer.eos_token_id
)

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated_text)
```

## Training Configuration
- **Temperature**: {config.get('temperature', 0.7)}
- **Max Length**: {config.get('max_length', 100)}
- **Training Method**: LoRA Fine-tuning

## Performance
Model performance metrics will be updated as evaluations are completed.

## Limitations
- This model inherits the limitations of its base model (GPT-2)
- Performance may vary depending on the input domain
- Generated content should be reviewed for appropriateness

## Ethical Considerations
Please use this model responsibly and be aware of potential biases in generated content.

---
*Model created using the LLM Development Platform*
"""

    def get_model_info(self, repo_name: str) -> Dict[str, Any]:
        """Get information about a model from HuggingFace Hub"""
        if not self.is_authenticated():
            return {"error": "Not authenticated"}
        
        try:
            # This would normally use the HuggingFace API
            return {
                "repo_name": repo_name,
                "url": f"https://huggingface.co/{repo_name}",
                "status": "Available for download"
            }
        except Exception as e:
            return {"error": str(e)}
    
    def list_user_models(self) -> List[Dict[str, Any]]:
        """List models uploaded by the current user"""
        if not self.is_authenticated():
            return []
        
        # This would normally fetch from HuggingFace API
        return []
    
    def delete_model(self, repo_name: str) -> Dict[str, Any]:
        """Delete a model from HuggingFace Hub"""
        if not self.is_authenticated():
            return {"success": False, "error": "Not authenticated"}
        
        try:
            # This would normally use the HuggingFace API to delete
            return {
                "success": True,
                "message": f"Model {repo_name} would be deleted (demo mode)"
            }
        except Exception as e:
            return {"success": False, "error": str(e)}

# Global service instance
huggingface_service = HuggingFaceService()